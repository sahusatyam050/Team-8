{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e76185-31f4-43a7-a21b-0332b10fcc1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Embedding Model (Sentence-Transformers)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74ffdecfa8b4dba82b506bb641c0fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Ingesting CSV: C:/Users/user/Downloads/employee_master.csv\n",
      "‚úÖ Saved 500 employees.\n",
      "üöÄ Ingesting JSON: C:/Users/user/Downloads/attendance_logs_detailed.json\n",
      "‚úÖ Attendance logs synchronized.\n",
      "üöÄ Vectorizing PDF: C:/Users/user/Downloads/Helix_Pro_Policy_v2.pdf\n",
      "‚úÖ Created 13 vector chunks in MongoDB.\n",
      "\n",
      "üî• SUCCESS: All Helix Corp data is now in MongoDB!\n",
      "Employees: 500\n",
      "Logs: 1\n",
      "Policy Vectors: 13\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- 1. CONFIGURATION & CONNECTIONS ---\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "client = pymongo.MongoClient(MONGO_URI)\n",
    "db = client[\"helix_hr_db\"]\n",
    "\n",
    "# Load a free, local embedding model (runs on your CPU/GPU)\n",
    "print(\"Loading Embedding Model (Sentence-Transformers)...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "# --- 2. STRUCTURED DATA (CSV) ---\n",
    "def ingest_csv(path):\n",
    "    print(f\"üöÄ Ingesting CSV: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    if 'join_date' in df.columns:\n",
    "        df['join_date'] = pd.to_datetime(df['join_date']).dt.strftime('%Y-%m-%d')\n",
    "    db.employees.delete_many({})\n",
    "    db.employees.insert_many(df.to_dict('records'))\n",
    "    print(f\"‚úÖ Saved {len(df)} employees.\")\n",
    "\n",
    "# --- 3. SEMI-STRUCTURED DATA (JSON) ---\n",
    "def ingest_json(path):\n",
    "    print(f\"üöÄ Ingesting JSON: {path}\")\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    db.attendance.delete_many({})\n",
    "    # If data is a dict, wrap in list; if list, insert directly\n",
    "    db.attendance.insert_many(data if isinstance(data, list) else [data])\n",
    "    print(f\"‚úÖ Attendance logs synchronized.\")\n",
    "\n",
    "# --- 4. UNSTRUCTURED DATA (PDF + VECTORIZATION) ---\n",
    "def ingest_pdf_vectors(path):\n",
    "    print(f\"üöÄ Vectorizing PDF: {path}\")\n",
    "    reader = PdfReader(path)\n",
    "    db.policy_vectors.delete_many({})\n",
    "    \n",
    "    chunk_count = 0\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        if not text.strip(): continue\n",
    "        \n",
    "        # Simple chunking: split by double newlines or large blocks\n",
    "        chunks = text.split('\\n\\n') \n",
    "        \n",
    "        for chunk in chunks:\n",
    "            if len(chunk.strip()) < 20: continue # Skip tiny fragments\n",
    "            \n",
    "            # Generate the vector embedding\n",
    "            vector = model.encode(chunk).tolist()\n",
    "            \n",
    "            # Store text + vector + metadata\n",
    "            doc = {\n",
    "                \"text\": chunk.strip(),\n",
    "                \"embedding\": vector,\n",
    "                \"metadata\": {\n",
    "                    \"source\": os.path.basename(path),\n",
    "                    \"page\": i + 1\n",
    "                }\n",
    "            }\n",
    "            db.policy_vectors.insert_one(doc)\n",
    "            chunk_count += 1\n",
    "            \n",
    "    print(f\"‚úÖ Created {chunk_count} vector chunks in MongoDB.\")\n",
    "\n",
    "# --- 5. EXECUTION ---\n",
    "csv_file = \"C:/Users/user/Downloads/employee_master.csv\"\n",
    "json_file = \"C:/Users/user/Downloads/attendance_logs_detailed.json\"\n",
    "pdf_file = \"C:/Users/user/Downloads/Helix_Pro_Policy_v2.pdf\"\n",
    "\n",
    "try:\n",
    "    ingest_csv(csv_file)\n",
    "    ingest_json(json_file)\n",
    "    ingest_pdf_vectors(pdf_file)\n",
    "    \n",
    "    print(\"\\nüî• SUCCESS: All Helix Corp data is now in MongoDB!\")\n",
    "    print(f\"Employees: {db.employees.count_documents({})}\")\n",
    "    print(f\"Logs: {db.attendance.count_documents({})}\")\n",
    "    print(f\"Policy Vectors: {db.policy_vectors.count_documents({})}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Critical Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ca17859-ca59-4c2e-8ca2-cca3af5c35a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Assistant updated! You can now use the Gradio chat without MongoDB Atlas errors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import redis\n",
    "\n",
    "# Connect to Redis\n",
    "r_cache = redis.Redis(host='localhost', port=6379, decode_responses=True)\n",
    "\n",
    "def helix_hr_assistant(user_query, employee_id=None):\n",
    "    # --- STEP 1: Check Redis Cache ---\n",
    "    try:\n",
    "        cached_response = r_cache.get(user_query)\n",
    "        if cached_response:\n",
    "            return f\"[CACHED RESPONSE] {cached_response}\"\n",
    "    except:\n",
    "        pass # If Redis is down, just continue to DB\n",
    "\n",
    "    # --- STEP 2: Local Vector Search (The Fix) ---\n",
    "    # Encode the user question\n",
    "    query_vector = model.encode(user_query)\n",
    "    \n",
    "    # Fetch all PDF chunks from your local Docker MongoDB\n",
    "    all_chunks = list(db.policy_vectors.find())\n",
    "    \n",
    "    if not all_chunks:\n",
    "        context = \"No policy documents found in the database.\"\n",
    "    else:\n",
    "        # Manual Cosine Similarity Calculation\n",
    "        def cosine_sim(v1, v2):\n",
    "            return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "        # Score every chunk against the query\n",
    "        for chunk in all_chunks:\n",
    "            chunk['score'] = cosine_sim(query_vector, chunk['embedding'])\n",
    "        \n",
    "        # Sort by best score and take the top 2 matches\n",
    "        top_matches = sorted(all_chunks, key=lambda x: x['score'], reverse=True)[:2]\n",
    "        context = \"\\n\".join([m['text'] for m in top_matches])\n",
    "\n",
    "    # --- STEP 3: Personal Data Lookup ---\n",
    "    personal_data = \"\"\n",
    "    if employee_id:\n",
    "        # Check CSV data for this employee\n",
    "        emp_record = db.employees.find_one({\"employee_id\": employee_id})\n",
    "        # Check JSON logs for this employee\n",
    "        attendance = list(db.attendance.find({\"employee_id\": employee_id}).limit(3))\n",
    "        \n",
    "        if emp_record:\n",
    "            personal_data = f\"Employee Record: {emp_record}\\nRecent Attendance: {attendance}\"\n",
    "\n",
    "    # --- STEP 4: Generate Real Response ---\n",
    "    # This calls your OpenAI GPT-4o function\n",
    "    try:\n",
    "        final_answer = get_llm_response(user_query, context, personal_data)\n",
    "        \n",
    "        # --- STEP 5: Save to Redis ---\n",
    "        r_cache.setex(user_query, 3600, final_answer)\n",
    "        return final_answer\n",
    "    except Exception as e:\n",
    "        return f\"Logic Error: {e}\"\n",
    "\n",
    "print(\"‚úÖ Assistant updated! You can now use the Gradio chat without MongoDB Atlas errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d097d0ba-81f2-4ba4-9369-d85e8db4724f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def chat_interface(query, emp_id):\n",
    "    return helix_hr_assistant(query, emp_id)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=chat_interface, \n",
    "    inputs=[\"text\", \"text\"], \n",
    "    outputs=\"text\",\n",
    "    title=\"Helix Corp HR AI Bot\",\n",
    "    description=\"Ask me about company policies or your attendance records.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b54425d5-baf6-464b-9b95-94dd2fc9d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set your key\n",
    "openai.api_key = \"\"\n",
    "\n",
    "def generate_grounded_response(user_query, context, personal_data):\n",
    "    prompt = f\"\"\"\n",
    "    You are the Helix Corp HR Assistant. Use the provided context and employee data to answer the query.\n",
    "    \n",
    "    GUIDELINES:\n",
    "    1. If the ans\n",
    "    prompt = f\"\"\"\n",
    "    You are the Helix Corp HR Assistant. Use the provided context and employee data to answer the query.\n",
    "    \n",
    "    GUIDELINES:\n",
    "    1. If the answer isn't in the context, say \"I don't have that information.\" \n",
    "    2. Do not hallucinate.\n",
    "    3. If a calculation is requested (like annual leave), show your step-by-step math based on the join date and policy.\n",
    "\n",
    "    CONTEXT FROM POLICY PDF:\n",
    "    {context}\n",
    "\n",
    "    EMPLOYEE DATA (CSV/JSON):\n",
    "    {personal_data}\n",
    "\n",
    "    USER QUERY: {user_query}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Corrected API call syntax\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise and helpful HR bot.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0 # Keep it factual\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Alias the function so it matches what your assistant calls\n",
    "generate_grounded_response = get_llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38741744-df6f-49fa-abd7-9c08dc8fb86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the missing library\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d07e2c-1aee-4524-80e1-6ac566105df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "646f7ba3-c684-4a92-8494-33e7b76a455d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Final System Check...\n",
      "‚úÖ MongoDB is Ready\n",
      "‚úÖ Redis is Ready\n",
      "‚úÖ Policy Data is Ready\n",
      "‚úÖ Employee Data is Ready\n",
      "üöÄ SYSTEM ONLINE: Helix HR Bot is ready for deployment.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging to a file\n",
    "logging.basicConfig(filename='helix_bot.log', level=logging.INFO)\n",
    "\n",
    "def validate_system_readiness():\n",
    "    print(\"üîç Final System Check...\")\n",
    "    checks = {\n",
    "        \"MongoDB\": db.command(\"ping\")[\"ok\"] == 1.0,\n",
    "        \"Redis\": r_cache.ping(),\n",
    "        \"Policy Data\": db.policy_vectors.count_documents({}) > 0,\n",
    "        \"Employee Data\": db.employees.count_documents({}) > 0\n",
    "    }\n",
    "    \n",
    "    for service, status in checks.items():\n",
    "        if status:\n",
    "            print(f\"‚úÖ {service} is Ready\")\n",
    "        else:\n",
    "            print(f\"‚ùå {service} is Missing/Empty\")\n",
    "            \n",
    "    return all(checks.values())\n",
    "\n",
    "# Run the check\n",
    "if validate_system_readiness():\n",
    "    print(\"üöÄ SYSTEM ONLINE: Helix HR Bot is ready for deployment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815abd5d-9765-45c0-a62c-b119e9ce813f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
